{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2d3b26-03c6-443c-b06d-53dd4d444aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.access.key\", \"AKIA3G3TGPEWAA535GBE\")\n",
    "spark.conf.set(\"fs.s3a.secret.key\", \"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\")\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.ap-south-1.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f09cc0-32be-4133-a086-b43c62aca5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"dbfs:/FileStore/data/\"\n",
    "def read_parquet(folder_name):\n",
    "    return spark.read.parquet(f\"{path}{folder_name}/\")\n",
    "\n",
    "\n",
    "existing_broker = read_parquet(\"existing_broker\")\n",
    "new_broker = read_parquet(\"new_broker\")\n",
    "retired_broker = read_parquet(\"retired_broker\")\n",
    "existing_client = read_parquet(\"existing_client\")\n",
    "new_client = read_parquet(\"new_client\")\n",
    "retired_client = read_parquet(\"retired_client\")\n",
    "existing_user = read_parquet(\"existing_user\")\n",
    "new_user = read_parquet(\"new_user\")\n",
    "retired_user = read_parquet(\"retired_user\")\n",
    "joined_df = read_parquet(\"cross_refer\")\n",
    "\n",
    "# existing_broker.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093d2752-4c6f-4d00-a49b-f1051657523a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf_options = {\n",
    "    \"sfUrl\":'NSSRVFW-CM48559.snowflakecomputing.com',   # Example: xy12345.west-europe.azure.snowflakecomputing.com\n",
    "    \"sfUser\":\"MBDKH1\", \n",
    "    \"sfPassword\":\"Snowflakeaccount4228\",\n",
    "    \"sfDatabase\":\"insurance_data\",\n",
    "    \"sfSchema\": \"insurance_gold\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"     # Optional but recommended\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86c8f2f-a91b-4871-bd12-b4eb5298d20a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_snowflake(df,target_table):\n",
    "    df.write\\\n",
    "    .format(\"snowflake\")\\\n",
    "    .options(**sf_options)\\\n",
    "    .option(\"dbtable\", target_table)\\\n",
    "    .mode(\"append\")\\\n",
    "    .save()\n",
    "\n",
    "write_to_snowflake(existing_broker, \"existing_broker\")\n",
    "write_to_snowflake(new_broker, \"new_broker\")\n",
    "write_to_snowflake(retired_broker, \"retired_broker\")\n",
    "write_to_snowflake(existing_client, \"existing_client\")\n",
    "write_to_snowflake(new_client, \"new_client\")\n",
    "write_to_snowflake(retired_client, \"retired_client\")\n",
    "write_to_snowflake(existing_user, \"existing_user\")\n",
    "write_to_snowflake(new_user, \"new_user\")\n",
    "write_to_snowflake(retired_user, \"retired_user\")\n",
    "write_to_snowflake(joined_df, \"joined_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e693ebb7-8d62-4f61-aa91-5c11a28ae2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5380342557373908>, line 5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3://projectuhc/uhc-silver-data/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m     df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\u001B[0;32m----> 5\u001B[0m write_to_s3_partitionBy(existing_broker, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting_broker\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m write_to_s3_partitionBy(new_broker, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_broker\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      7\u001B[0m write_to_s3_partitionBy(retired_broker, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretired_broker\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'existing_broker' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'existing_broker' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'existing_broker' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5380342557373908>, line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3://projectuhc/uhc-silver-data/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m     df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n\u001B[0;32m----> 5\u001B[0m write_to_s3_partitionBy(existing_broker, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting_broker\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m write_to_s3_partitionBy(new_broker, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_broker\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m write_to_s3_partitionBy(retired_broker, path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretired_broker\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'existing_broker' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write_to_s3_partitionBy(df, path, target_table):\n",
    "    path = f\"s3://projectuhc/uhc-silver-data/{target_table}\"\n",
    "    df.write.format(\"parquet\").mode(\"append\").save(path)\n",
    "\n",
    "write_to_s3_partitionBy(existing_broker, path, \"existing_broker\")\n",
    "write_to_s3_partitionBy(new_broker, path, \"new_broker\")\n",
    "write_to_s3_partitionBy(retired_broker, path, \"retired_broker\")\n",
    "write_to_s3_partitionBy(existing_client, path, \"existing_client\")\n",
    "write_to_s3_partitionBy(new_client, path, \"new_client\")\n",
    "write_to_s3_partitionBy(retired_client, path, \"retired_client\")\n",
    "write_to_s3_partitionBy(existing_user, path, \"existing_user\")\n",
    "write_to_s3_partitionBy(new_user, path, \"new_user\")\n",
    "write_to_s3_partitionBy(retired_user, path, \"retired_user\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66137e1a-8558-49d8-bb26-28b6c51709c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write_to_s3_partitionby(df, path, target_table):\n",
    "    path = f\"s3://insurance-data-112/gold_insurance/{target_table}.parquet\"\n",
    "    df.write.format(\"parquet\").partitionBy(\"plan_id\",\"broker_id\",\"client_id\").mode(\"append\").save(path)\n",
    "\n",
    "write_to_s3_partitionby(joined_df, path, \"joined_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a2d5c0-81f6-4849-acce-cb2c51026fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6816575291639016>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m joined_df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/dataframe.py:2363\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2361\u001B[0m \u001B[38;5;129m@property\u001B[39m\n",
       "\u001B[1;32m   2362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m-> 2363\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n",
       "\u001B[1;32m   2364\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2365\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n",
       "\u001B[1;32m   2366\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on shared clusters. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations#shared-access-mode-limitations-on-unity-catalog"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkNotImplementedError",
        "evalue": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on shared clusters. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations#shared-access-mode-limitations-on-unity-catalog"
       },
       "metadata": {
        "errorSummary": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on shared clusters. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations#shared-access-mode-limitations-on-unity-catalog"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOT_IMPLEMENTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)",
        "File \u001B[0;32m<command-6816575291639016>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m joined_df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/dataframe.py:2363\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2361\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 2363\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n\u001B[1;32m   2364\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2365\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m   2366\u001B[0m     )\n",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on shared clusters. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations#shared-access-mode-limitations-on-unity-catalog"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to get number of partitions\n",
    "joined_df.rdd.getNumPartitions()\n",
    "#to change partition size to 200 mb\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 209715200)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "job2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}